{
  "$schema": "https://json.schemastore.org/claude-code-hooks.json",
  "description": "autonomous-stan workflow hooks: context injection, test tracking, phase enforcement, criteria evaluation",
  "hooks": {
    "UserPromptSubmit": [
      {
        "hooks": [
          {
            "type": "command",
            "command": "python3 ${CLAUDE_PLUGIN_ROOT}/hooks/autonomous-stan/stan_context.py",
            "timeout": 10
          }
        ]
      }
    ],
    "PreToolUse": [
      {
        "matcher": "Bash",
        "hooks": [
          {
            "type": "command",
            "command": "python3 ${CLAUDE_PLUGIN_ROOT}/hooks/autonomous-stan/stan_gate.py",
            "timeout": 10
          },
          {
            "type": "command",
            "command": "python3 ${CLAUDE_PLUGIN_ROOT}/hooks/autonomous-stan/git_guard.py",
            "timeout": 10
          },
          {
            "type": "command",
            "command": "python3 ${CLAUDE_PLUGIN_ROOT}/hooks/autonomous-stan/credential_guard.py",
            "timeout": 10
          }
        ]
      },
      {
        "matcher": "*",
        "hooks": [
          {
            "type": "command",
            "command": "python3 ${CLAUDE_PLUGIN_ROOT}/hooks/autonomous-stan/research_guard.py"
          }
        ]
      }
    ],
    "PostToolUse": [
      {
        "matcher": "Bash",
        "hooks": [
          {
            "type": "command",
            "command": "python3 ${CLAUDE_PLUGIN_ROOT}/hooks/autonomous-stan/stan_track.py",
            "timeout": 10
          },
          {
            "type": "command",
            "command": "python3 ${CLAUDE_PLUGIN_ROOT}/hooks/autonomous-stan/loop_breaker.py",
            "timeout": 10
          }
        ]
      },
      {
        "matcher": "Edit|Write|MultiEdit",
        "hooks": [
          {
            "type": "command",
            "command": "python3 ${CLAUDE_PLUGIN_ROOT}/hooks/autonomous-stan/loop_breaker.py",
            "timeout": 10
          }
        ]
      },
      {
        "matcher": "Edit|Write",
        "hooks": [
          {
            "type": "prompt",
            "prompt": "You are the STAN EVALUATOR - an independent quality checker.\n\nThe main agent just edited a file. Analyze critically:\n\n1. If a checkbox was marked complete: Is the criterion ACTUALLY met? Look for real evidence, not just the checkbox state.\n2. If code was written: Is it complete? Any TODOs, placeholder comments, missing imports, unfinished logic?\n3. If acceptance criteria were checked: Does the actual implementation genuinely match?\n4. If a document was written: Is it concrete and actionable? Watch for vague statements, missing specifics, aspirational language without substance.\n\n5. CRITICAL \u2014 RESEARCH VERIFICATION:\n   Does this edit contain technical claims, architecture decisions, specific values (CSS, config, versions), or technology choices?\n   If YES: Were these RESEARCHED or GUESSED from training knowledge?\n   - Check: Were web_search, context7, or graphiti used BEFORE this edit in the conversation?\n   - If no research evidence: FLAG IT. Say: \"\u26a0\ufe0f UNRESEARCHED CLAIM: [specific claim]. This needs verification from current sources before it can be finalized.\"\n   - Multiple unresearched claims = BLOCK recommendation: \"\ud83d\uded1 Too many unverified claims. Research first, write second.\"\n\n   Examples of claims that MUST be researched:\n   - CSS values (clamp, font-size, spacing) \u2192 Utopia.fyi or similar tools\n   - Framework config \u2192 Context7 live docs\n   - API endpoints/versions \u2192 Official docs\n   - Architecture patterns \u2192 Multiple sources confirming\n   - Library recommendations \u2192 Verify they exist and are current\n   - Performance claims \u2192 Benchmarks or documentation\n\nBe skeptical. The main agent has self-serving bias, tends to check boxes prematurely, AND tends to guess technical facts instead of researching them.\n\nIf you find issues, state them specifically so the agent can fix them.\nIf the edit is legitimate, researched, and complete, confirm briefly.",
            "timeout": 30
          }
        ]
      }
    ],
    "Stop": [
      {
        "hooks": [
          {
            "type": "prompt",
            "prompt": "You are the STAN GATE - final quality barrier before task completion.\n\nReview the transcript for this session. Before allowing completion, verify:\n\n1. ALL acceptance criteria are genuinely met (not just checked off)\n2. No TODO comments or incomplete implementations remain\n3. If tests were required, they must have passed (look for test output in transcript)\n4. Document quality matches the stated requirements\n5. If a PRD was created: Does it have concrete goals, measurable criteria, and real evidence?\n6. If code was written: Is it tested, linted, and complete?\n\n7. CRITICAL \u2014 RESEARCH AUDIT:\n   Scan ALL technical claims, architecture decisions, config values, and technology choices in the output.\n   For each: Is there evidence of research (web_search, context7, graphiti, docs) in the transcript?\n   - Unresearched claims MUST be flagged: \"\u26a0\ufe0f UNVERIFIED: [claim] \u2014 no research evidence found\"\n   - If >3 unresearched technical claims: REJECT completion. \"\ud83d\uded1 Multiple unverified claims. The agent guessed instead of researching.\"\n\nThis is NOT just for code. Check documents, plans, research - everything the agent produced.\nTechnical accuracy matters more than completeness. A half-finished researched document beats a complete guessed one.\n\nIf everything is genuinely complete AND researched, confirm approval.\nIf issues remain, list them specifically so the agent can address them before finishing.",
            "timeout": 30
          },
          {
            "type": "prompt",
            "prompt": "DEVIL'S ADVOCATE GATE — AUTOMATIC ENFORCEMENT\n\nBefore completing, check if the document frontmatter contains BOTH:\n  - devils-advocate\n  - devils-advocate-verify\nin the techniques_applied list.\n\nIf BOTH are present → You're clear. Say nothing about DA.\n\nIf MISSING → You MUST run the DA review NOW. Do not ask the user. Do not suggest commands. Just DO IT:\n\n1. Determine the phase from what was produced:\n   - PRD/requirements → DEFINE phase\n   - Plan/architecture → PLAN phase\n   - Code/implementation → CREATE phase\n\n2. Spawn a Task (sub-agent) for Pass 1 with this prompt:\n   \"You are a [ROLE] performing a Devil's Advocate review. Read techniques/references/da-role-[role].md for your full method. Then review the work product in docs/. Output your findings using the template in the reference file. Be specific, be tough, be fair.\"\n\n   Phase → Roles:\n   - DEFINE: Pass 1 = Analyst (da-role-analyst.md), Pass 2 = Product Manager (da-role-pm.md)\n   - PLAN: Pass 1 = Architect (da-role-architect.md), Pass 2 = Security Reviewer (da-role-security.md)\n   - CREATE: Pass 1 = Senior Developer (da-role-dev.md), Pass 2 = QA Engineer (da-role-qa.md)\n\n3. Address the findings from Pass 1.\n\n4. Spawn a Task for Pass 2 with the second role. This role also verifies Pass 1 fixes.\n\n5. Address Pass 2 findings.\n\n6. Add BOTH entries to the document frontmatter:\n   techniques_applied:\n     - devils-advocate\n     - devils-advocate-verify\n\n7. THEN you may complete.\n\nThis is NOT optional. The gate in stan_gate.py will block your commit anyway if these entries are missing. Do the review proactively instead of hitting the wall.",
            "timeout": 120
          }
        ]
      }
    ]
  }
}